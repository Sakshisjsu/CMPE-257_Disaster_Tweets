{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dda977-c0cd-4d87-a5fc-5789928fe4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sonali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn import model_selection, metrics, preprocessing, ensemble, model_selection, metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Dropout, Input, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32096a2b-eaea-425f-b0ef-82a3ddcc099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94fc5d7-c8d7-4901-affc-802e3056d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of missing data for column keyword:  61\n",
      "No. of missing data for column location:  2533\n",
      "No. of missing data for column text:  0\n",
      "No. of missing data for column target:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of missing data for column keyword: \", train['keyword'].isna().sum())\n",
    "print(\"No. of missing data for column location: \", train['location'].isna().sum())\n",
    "print(\"No. of missing data for column text: \", train['text'].isna().sum())\n",
    "print(\"No. of missing data for column target: \", train['target'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5a6aaf-4ee9-499a-ba98-e13d9e5f2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text columns\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def clean_text(each_text):\n",
    "\n",
    "    # remove URL from text\n",
    "    each_text_no_url = re.sub(r\"http\\S+\", \"\", each_text)\n",
    "    \n",
    "    # remove numbers from text\n",
    "    text_no_num = re.sub(r'\\d+', '', each_text_no_url)\n",
    "\n",
    "    # tokenize each text\n",
    "    word_tokens = word_tokenize(text_no_num)\n",
    "    \n",
    "    # remove sptial character\n",
    "    clean_text = []\n",
    "    for word in word_tokens:\n",
    "        clean_text.append(\"\".join([e for e in word if e.isalnum()]))\n",
    "\n",
    "    # remove stop words and lower\n",
    "    text_with_no_stop_word = [w.lower() for w in clean_text if not w in stop_words]  \n",
    "\n",
    "    # do stemming\n",
    "    stemmed_text = [stemmer.stem(w) for w in text_with_no_stop_word]\n",
    "    \n",
    "    return \" \".join(\" \".join(stemmed_text).split())\n",
    "\n",
    "\n",
    "train['clean_text'] = train['text'].apply(lambda x: clean_text(x) )\n",
    "train['keyword'] = train['keyword'].fillna(\"none\")\n",
    "train['clean_keyword'] = train['keyword'].apply(lambda x: clean_text(x) )\n",
    "\n",
    "# Combining 'clean_keyword' and 'clean_text' into one column\n",
    "train['keyword_text'] = train['clean_keyword'] + \" \" + train[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c668bc64-80c6-42fc-980d-33b0903754d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'keyword_text'\n",
    "label = \"target\"\n",
    "\n",
    "# split train and test\n",
    "X_train, X_test,y_train, y_test = model_selection.train_test_split(train[feature],\n",
    "                                                                   train[label],\n",
    "                                                                   test_size=0.3,\n",
    "                                                                   random_state=0, \n",
    "                                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f1ac77-459e-4709-a48b-16913cfdf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_GBC = X_train.values.reshape(-1)\n",
    "x_test_GBC = X_test.values.reshape(-1)\n",
    "\n",
    "# Vectorizing text\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_GBC = vectorizer.fit_transform(X_train_GBC)\n",
    "x_test_GBC = vectorizer.transform(x_test_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b2bd83-e9ac-411a-a94d-8b58579e1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=9, max_features=8, min_samples_leaf=2,\n",
       "                           min_samples_split=6, n_estimators=2000,\n",
       "                           subsample=0.9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model = ensemble.GradientBoostingClassifier(learning_rate=0.1,                                            \n",
    "                                            n_estimators=2000,\n",
    "                                            max_depth=9,\n",
    "                                            min_samples_split=6,\n",
    "                                            min_samples_leaf=2,\n",
    "                                            max_features=8,\n",
    "                                            subsample=0.9)\n",
    "model.fit(X_train_GBC, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6115335-348f-4153-b142-993e4fdbd352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8025394045534151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84      1338\n",
      "           1       0.82      0.67      0.74       946\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.81      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n",
      "Test F-scoare:  0.7385507246376811\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predicted_prob = model.predict_proba(x_test_GBC)[:,1]\n",
    "predicted = model.predict(x_test_GBC)\n",
    "\n",
    "accuracy = metrics.accuracy_score(predicted, y_test)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "print(metrics.classification_report(y_test, predicted, target_names=[\"0\", \"1\"]))\n",
    "print(\"Test F-scoare: \", metrics.f1_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd98a11-d67a-4542-872c-255b844f3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['clean_text'] = test['text'].apply(lambda x: clean_text(x) )\n",
    "test['keyword'] = test['keyword'].fillna(\"none\")\n",
    "test['clean_keyword'] = test['keyword'].apply(lambda x: clean_text(x) )\n",
    "\n",
    "# Combining 'clean_keyword' and 'clean_text' into one column\n",
    "test['keyword_text'] = test['clean_keyword'] + \" \" + test[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63a574f-d993-4b65-878b-5fd670281460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_GBC = train[feature].values.reshape(-1)\n",
    "test_GBC = test[feature].values.reshape(-1)\n",
    "\n",
    "# Vectorizing text\n",
    "vectorizer = CountVectorizer()\n",
    "train_GBC = vectorizer.fit_transform(train_GBC)\n",
    "test_GBC = vectorizer.transform(test_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf58491-6994-4d4d-af8f-685c33143bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=9, max_features=8, min_samples_leaf=2,\n",
       "                           min_samples_split=6, n_estimators=2000,\n",
       "                           subsample=0.9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "modelGb = ensemble.GradientBoostingClassifier(learning_rate=0.1,                                            \n",
    "                                            n_estimators=2000,\n",
    "                                            max_depth=9,\n",
    "                                            min_samples_split=6,\n",
    "                                            min_samples_leaf=2,\n",
    "                                            max_features=8,\n",
    "                                            subsample=0.9)\n",
    "modelGb.fit(train_GBC, train[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace9503d-a874-41db-aa25-23543565fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedGb = modelGb.predict(test_GBC)\n",
    "\n",
    "sub_sample = pd.read_csv('sample_submission.csv')\n",
    "submit = sub_sample.copy()\n",
    "submit.target = predictedGb\n",
    "submit.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c20804-d65f-49f7-93f6-0c4be35a6768",
   "metadata": {},
   "source": [
    "### LSTM-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f19ffd6-9f66-4b66-ae73-85e6d1196bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters\n",
    "path_to_glove_file = './glove.6B/glove.6B.300d.txt' #link: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "embedding_dim = 300\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1024\n",
    "epochs = 20\n",
    "sequence_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "780d20d1-545e-474b-81d8-41a373d3d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Y shape: (7613, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define train and test labels\n",
    "# y_train_LSTM = y_train.values.reshape(-1,1)\n",
    "# y_test_LSTM = y_test.values.reshape(-1,1)\n",
    "\n",
    "# print(\"Training Y shape:\", y_train_LSTM.shape)\n",
    "# print(\"Testing Y shape:\", y_test_LSTM.shape)\n",
    "\n",
    "train_LSTM = train[label].values.reshape(-1,1)\n",
    "\n",
    "print(\"Training Y shape:\", train_LSTM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eafcf71e-4333-4e3a-8e42-7029a47ae6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  7\n"
     ]
    }
   ],
   "source": [
    "# Tokenize train data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocabulary Size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebbc8b3c-c8e7-4ee3-8865-a869e1973aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape:  (7613, 100)\n",
      "Testing X shape:  (3263, 100)\n"
     ]
    }
   ],
   "source": [
    "# Pad train and test \n",
    "X_trainl = pad_sequences(tokenizer.texts_to_sequences(train[feature]), maxlen=sequence_len)\n",
    "X_testl = pad_sequences(tokenizer.texts_to_sequences(test[feature]), maxlen=sequence_len)\n",
    "\n",
    "print(\"Training X shape: \", X_trainl.shape)\n",
    "print(\"Testing X shape: \", X_testl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fa311b4-09ce-4415-acd4-0d311a399904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Read word embeddings\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04d94112-c7c6-4f8f-ab24-b714c60c3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layer in Keras\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                            embedding_dim,\n",
    "                                            weights=[embedding_matrix],\n",
    "                                            input_length=sequence_len,\n",
    "                                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89b844e1-a96d-4707-a068-6247d132c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 300)          2100      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           192128    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 852,149\n",
      "Trainable params: 850,049\n",
      "Non-trainable params: 2,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "sequence_input = Input(shape=(sequence_len, ), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedding_sequences)\n",
    "x = Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "modelLSTM = Model(sequence_input, outputs)\n",
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc41dea6-9441-4041-a0b5-a8ef5d91f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model\n",
    "modelLSTM.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afe14791-4597-40b1-9397-d843c2d105f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 47s 4s/step - loss: 0.6886 - accuracy: 0.5701\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 32s 4s/step - loss: 0.6844 - accuracy: 0.5703\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 37s 4s/step - loss: 0.6833 - accuracy: 0.5703\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 31s 4s/step - loss: 0.6824 - accuracy: 0.5703\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 28s 3s/step - loss: 0.6828 - accuracy: 0.5706\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 44s 6s/step - loss: 0.6827 - accuracy: 0.5710\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 41s 5s/step - loss: 0.6824 - accuracy: 0.5702\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 29s 4s/step - loss: 0.6827 - accuracy: 0.5706\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.6828 - accuracy: 0.5703\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.6831 - accuracy: 0.5706\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 55s 7s/step - loss: 0.6829 - accuracy: 0.5713\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 36s 4s/step - loss: 0.6826 - accuracy: 0.5713\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 58s 8s/step - loss: 0.6829 - accuracy: 0.5699\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.6830 - accuracy: 0.5709\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 79s 10s/step - loss: 0.6822 - accuracy: 0.5711\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 37s 5s/step - loss: 0.6828 - accuracy: 0.5703\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 30s 4s/step - loss: 0.6826 - accuracy: 0.5703\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 32s 4s/step - loss: 0.6830 - accuracy: 0.5705\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 50s 6s/step - loss: 0.6826 - accuracy: 0.5709\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 32s 4s/step - loss: 0.6825 - accuracy: 0.5707\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM Model\n",
    "history = modelLSTM.fit(X_trainl,\n",
    "                    train[label],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1d7b1d8-16e0-4536-89a8-13692abcf0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictedl = modelLSTM.predict(X_testl, verbose=1, batch_size=10000)\n",
    "\n",
    "y_predictedl = [1 if each > 0.5 else 0 for each in predictedl]\n",
    "\n",
    "submit = sub_sample.copy()\n",
    "submit.target = y_predictedl\n",
    "submit.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a119ae4-b113-43f5-b9eb-94511b1eac0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
